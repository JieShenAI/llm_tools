{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "# llm =  Ollama(model=\"deepseek-r1:32b\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载本地的GDP数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(file, encoding=\"utf-8\"):\n",
    "    with open(file, \"r\", encoding=encoding) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def save_to_json(obj, file, encoding=\"utf-8\"):\n",
    "    parent_fold = os.path.dirname(file)\n",
    "    os.makedirs(parent_fold, exist_ok=True)\n",
    "    with open(file, \"w\", encoding=encoding) as f:\n",
    "        json.dump(obj, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_json(\"../data/长沙_gdp.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"请你阅读长沙市的政府工作报告文本节选，抽取其中GDP具体的值。\n",
    "首先会在前面的文中提到全年（表述可能为：全年、今年、去年）的国民生产总值的具体数据，然后会在后面的文本展望未来预期的国民生产总值。\n",
    "如果文中出现了多个对GDP值的表述请你输出前面的。不要返回政府对未来预期的GDP的值。\n",
    "\n",
    "文本节选如下：\n",
    "{{text}}\n",
    "\n",
    "返回json格式:\n",
    "```json\n",
    "{\n",
    "    \"reason\":\"说明理由\",\n",
    "    \"GDP\": \"GDP值\"\n",
    "}\n",
    "```\n",
    "GDP值为数字，不需要单位，若文中未提到GDP值，请填写\"<not found>\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Please install OpenAI SDK first: `pip3 install openai`\n",
    "\n",
    "# from openai import OpenAI\n",
    "\n",
    "# client = OpenAI(base_url=\"http://localhost:11434/v1\")\n",
    "\n",
    "# response = client.chat.completions.create(\n",
    "#     model=\"deepseek-r1:32b\",\n",
    "#     messages=[\n",
    "#         {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "#         {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "#     ],\n",
    "#     stream=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 解析大模型抽取结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': [], 'label': '<not found>', 'year': '2003', 'city': '长沙'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['长沙市2003.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_pred(llm, dataset, output_file):\n",
    "    pred_dataset = []\n",
    "    for item in tqdm(dataset.values()):\n",
    "        if item[\"text\"] == []:\n",
    "            continue\n",
    "        text = \"\\n\".join(item[\"text\"])\n",
    "        prompt = PROMPT_TEMPLATE.replace(\"{{text}}\", text)\n",
    "        item[\"prompt\"] = prompt\n",
    "        llm_answer = llm.invoke(prompt)\n",
    "        item[\"llm_answer\"] = llm_answer\n",
    "        llm_response_text = re.search(\n",
    "            r\"```json\\n(.*)\\n```\", llm_answer, re.DOTALL\n",
    "        ).group(1)\n",
    "        reponse_d = json.loads(llm_response_text)\n",
    "        pred_label = reponse_d.get(\"GDP\", \"error\")\n",
    "        item[\"pred_label\"] = pred_label\n",
    "        pred_dataset.append(item)\n",
    "    save_to_json(pred_dataset, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:59<00:00,  2.81s/it]\n"
     ]
    }
   ],
   "source": [
    "qwen_7b =  Ollama(model=\"qwen2.5:7b\", temperature=0)\n",
    "llm_pred(qwen_7b, dataset, \"output/长沙_gdp_qwen_7b.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [04:45<00:00, 13.59s/it]\n"
     ]
    }
   ],
   "source": [
    "deepseek_r1_32b =  Ollama(model=\"deepseek-r1:32b\", temperature=0)\n",
    "llm_pred(deepseek_r1_32b, dataset, \"output/长沙_gdp_deepseek_r1_32b.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [01:43<00:00,  4.92s/it]\n"
     ]
    }
   ],
   "source": [
    "deepseek_r1_7b = Ollama(model=\"deepseek-r1:7b\", temperature=0)\n",
    "llm_pred(deepseek_r1_7b, dataset, \"output/长沙_gdp_deepseek_r1_7b.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [08:34<00:00, 24.50s/it]\n"
     ]
    }
   ],
   "source": [
    "qwq_32b = Ollama(model=\"qwq:32b\", temperature=0)\n",
    "llm_pred(qwq_32b, dataset, \"output/长沙_gdp_qwq_32b.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
